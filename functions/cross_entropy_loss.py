"""
交叉熵损失函数(Cross Entropy Loss Function)
"""

"""
1.信息量(self-information,或者称为 自信息)
公式: I(x)=−lnP(x),P(x)是概率,取值为[0,1]。
调用函数 pic_self_information() 绘制图像观察,当P(x)=0时 I(x)=∞；当P(x)=1时 I(x)=0。
也就是说概率越小的事件包含越多的信息,而必然发生的事件不包含任何(有效)信息。

2. 香农熵(Shannon entropy)
设 X 是一个有限个值的离散随机变量,其概率分布为：P(X=xi)=P(i), i=1,2,…,n
使用信息量的期望来量化整个概率分布中的不确定性总量:
H(P)=-∑(P(i) * lnP(i))
使用抛硬币的例子解释: 假设正面朝上的概率为p,即 P(X=正) = p,则反面朝上的概率为1-p,即 P(X=反) = 1-p
香农熵大小为：H(P)=-(p*ln(p) + (1-p)*ln(1-p)),调用函数 pic_coin_shannon_entropy() 绘制图像
假设我们取 P(X=正)=1,代入可得 H(p) = -(1*ln(1) + 0*ln(0)) = -(0 - NaN) = 0
假设我们取 P(X=正)=0.7,代入可得 H(p) = -(0.7*ln(0.7) + 0.3*ln(0.3)) = -(-0.25 -0.36) = 0.61
如何理解呢?
当正面朝上的概率 p=1.0 时,此时香农熵H(p)=0,随机变量完全没有不确定性,硬币肯定朝上;
当正面朝上的概率 p=0.7 时,此时香农熵较大,随机变量不确定性也较大,硬币不一定朝上。
说白了,香农熵就是代表了事件发生的不确定性,不确定性越大,香农熵越大,不确定性越小,香农熵越小。

3. KL散度(Kullback-Leibler divergence),也叫做相对熵
如果对于同一个随机变量X有 两个单独的概率分布P(x)和Q(x), 则我们可以使用KL散度来衡量这两个概率分布之间的差异:
其公式定义如下：
D(P,Q)=∑( P(xi) * ln( P(xi)/Q(xi) ) )
在机器学习中,常常使用P(x)来表示样本的真实分布,Q(x)来表示模型所预测的分布。
比如在一个三分类任务中一张图片真实分布 P(X)=[1,0,0],预测分布Q(X)=[0.7,0.2,0.1]
计算 KL 散度: 1*ln(1/0.7) + 0 + 0 = 0.36
KL 散度越小越好,如果 Q(X)=[1,0,0],那么KL散度=0,此时预测结果 100% 准确

4. 交叉熵(cross-entropy)
通过上面可知可以通过 最小化相对熵 使得 预测分布Q 逼近 真实分布P。
改写下相对熵的公式(将除法写为减法)
D(P,Q)= ∑( P(xi) * ln( P(xi)/Q(xi) ) )
      = ∑(P(xi)*lnP(xi)) - ∑(P(xi)*lnQ(xi))
其中 ∑(P(xi)*lnP(xi)) 就是香农熵的负数,对于确定的分布P,其香农熵H(P) 是一个常数,做最小化处理时没有影响
因此我们将公式的后半部分 -∑(P(xi)*lnQ(xi)) 记为H(P,Q),称做 交叉熵。
让我们再一次计算上面的例子：一张图片真实分布 P(X)=[1,0,0],预测分布Q(X)=[0.7,0.2,0.1]
H(P,Q) = -(1*ln(0.7) + 0 + 0) = 0.36
相比于 KL 散度,计算交叉熵更容易(少了一步除法),所以在机器学习中常常使用交叉熵做为损失函数。

5.总结：
交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度,在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小,模型预测效果就越好。
交叉熵在分类问题中常常与 softmax 是标配,softmax将输出的结果进行处理,使其多个分类的预测值和为1,再通过交叉熵来计算损失。
"""


def pic_self_information():
    """
    信息量图像绘制
    TODO 待补充
    """
    pass


def pic_coin_shannon_entropy():
    """
    抛硬币信息熵图像绘制
    TODO 待补充
    """
    pass
